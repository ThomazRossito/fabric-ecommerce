# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "ff4c5f56-7490-4a4b-a0b1-115bd42aaaf2",
# META       "default_lakehouse_name": "TARN_LH_DEV",
# META       "default_lakehouse_workspace_id": "e1e7a468-d318-49d0-b238-287c09020d88",
# META       "known_lakehouses": [
# META         {
# META           "id": "ff4c5f56-7490-4a4b-a0b1-115bd42aaaf2"
# META         }
# META       ]
# META     }
# META   }
# META }

# MARKDOWN ********************

# # üìä Monitoramento: Log de Execu√ß√£o do Pipeline Medallion
# 
# ## Descri√ß√£o
# 
# Este notebook √© a central de governan√ßa e observabilidade do projeto. Ele consolida os metadados de execu√ß√£o capturados pelo Orquestrador e os persiste em uma tabela Delta para an√°lise hist√≥rica e dashboards de monitoramento.
# 
# ### Funcionalidades:
# 
# 1. **üì• Captura de Metadados** - Processa os resultados de todas as camadas (Bronze ‚Üí Gold)
# 2. **üìà M√©tricas de Performance** - Registra o tempo exato de cada atividade
# 3. **üîç Gest√£o de Erros** - Armazena mensagens detalhadas para facilitar o troubleshooting
# 4. **üíæ Persist√™ncia Delta** - Mant√©m o hist√≥rico completo de execu√ß√µes para o Power BI
# 
# ### Detalhes do Log:
# 
# - `run_id`: Identificador √∫nico da execu√ß√£o
# - `nome_notebook`: Identifica qual etapa foi executada
# - `registros_processados`: Volume de dados por tabela
# - `duracao_segundos`: Performance granular por atividade
# - `mensagem_erro`: Descri√ß√£o t√©cnica de poss√≠veis falhas
# 
# ### Arquitetura de Monitoramento:
# ```
# Orchestrator (results) ‚Üí notebook_monitoramento_log ‚Üí monitoramento.tb_monitoramento_log (Delta)
# ```


# MARKDOWN ********************

# ## üë®‚Äçüíª **Autor** üë®‚Äçüíª
# 
# > **Estruturado por:** <span style="font-size: 1.5em;">Thomaz Antonio Rossito Neto</span>
# 
# <b><span style="font-size: 1.2em; font-style: italic;">üèÜ Profissional Certificado Databricks</span></b>
# 
# <div style="display: flex; flex-wrap: wrap; align-items: center; margin-bottom: 20px;">
#     <img src="https://api.accredible.com/v1/frontend/credential_website_embed_image/badge/125134719" width="135" style="margin-right: -25px;"/>
#     <img src="https://api.accredible.com/v1/frontend/credential_website_embed_image/badge/167127257" width="135" style="margin-right: -25px;"/>
#     <img src="https://api.accredible.com/v1/frontend/credential_website_embed_image/badge/169321258" width="135" style="margin-right: -25px;"/>
#     <img src="https://api.accredible.com/v1/frontend/credential_website_embed_image/badge/125134780" width="135" style="margin-right: -25px;"/>
#     <img src="https://api.accredible.com/v1/frontend/credential_website_embed_image/badge/157011932" width="135"/>
# </div>
# 
# <div style="display: flex; flex-wrap: wrap; align-items: center;">
#     <img src="https://images.credly.com/images/af27ef78-6967-4082-b6ce-8111b1af47e1/MTA_Database_Fundamentals-01.png" width="115" style="margin-right: 10px;"/>
#     <img src="https://images.credly.com/size/340x340/images/70eb1e3f-d4de-4377-a062-b20fb29594ea/azure-data-fundamentals-600x600.png" width="115" style="margin-right: 10px;"/>
#     <img src="https://images.credly.com/images/bb4a3c26-9f24-4913-9ae5-7331a3d657a6/MCSA-Data-Engineering-with-Azure_2019.png" width="115" style="margin-right: 10px;"/>
#     <img src="https://images.credly.com/images/7e080b6a-0494-4b3e-a016-23f73566495f/MCSE-Data-Management-and-Analytics_2019.png" width="115"/>
# </div>
# 
# <br>
# 
# [Certifica√ß√µes Databricks](https://credentials.databricks.com/profile/thomazantoniorossitoneto39867/wallet)              
# [Certifica√ß√µes Microsoft](https://www.credly.com/users/thomaz-antonio-rossito-neto/badges#credly)
# 
# ---
# 
# ### Data de Cria√ß√£o
# ##### Fevereiro 2026


# MARKDOWN ********************

# ## ‚öôÔ∏è Configura√ß√µes e Par√¢metros
# 
# Defini√ß√£o dos par√¢metros de entrada que o Orquestrador enviar√° e configura√ß√£o dos imports necess√°rios.ros

# CELL ********************

%run utils

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# PARAMETERS CELL ********************

dag_results_raw = "" # Receber√° o JSON do orquestrador
run_id = ""          # ID opcional da execu√ß√£o

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Bibliotecas

# CELL ********************

import json
import ast
from datetime import datetime
from pyspark.sql.types import (
    StructType, 
    StructField, 
    StringType, 
    BooleanType, 
    DoubleType, 
    LongType
)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Tratamento de Entrada
# 
# Converte a string JSON recebida para um dicion√°rio Python utiliz√°vel

# CELL ********************

print("üíæ Iniciando Persist√™ncia de Logs de Monitoramento...")

if isinstance(dag_results_raw, str) and dag_results_raw.strip() != "":
    try:
        results_dict = json.loads(dag_results_raw)
    except:
        # Fallback para aspas simples (ast) se o json falhar
        results_dict = ast.literal_eval(dag_results_raw)
else:
    results_dict = dag_results_raw

# Configura√ß√µes de Destino
schema_name = "monitoramento"
table_name = "tb_monitoramento_log"

spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## üîÑ Processamento e Persist√™ncia
# 
# L√≥gica para transformar o dicion√°rio de resultados em um DataFrame estruturado e salvar na tabela Delta de monitoramento.

# CELL ********************

print(f"üöÄ Prepara√ß√£o de dados para o DataFrame...")

log_data = []
timestamp_monitoramento = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

if isinstance(results_dict, dict):
    for activity, detail in results_dict.items():
        # Extrai o nome da camada (Bronze/Silver/Gold) para filtros no dashboard
        camada_nome = activity.split('_')[1] if '_' in activity else activity
        
        log_data.append({
            "run_id": str(run_id),
            "nome_notebook": str(activity),
            "status": "Success" if detail.get("success") else "Failed",
            "sucesso": bool(detail.get("success", False)),
            "registros_processados": int(detail.get("records", 0)),
            "duracao_segundos": float(detail.get("duration", 0)),
            "mensagem_erro": str(detail.get("error", "")),
            "camada": str(camada_nome),
            "timestamp_execucao": timestamp_monitoramento
        })

# Defini√ß√£o de Schema expl√≠cito para performance e tipagem correta
schema = StructType([
    StructField("run_id", StringType(), True),
    StructField("nome_notebook", StringType(), True),
    StructField("status", StringType(), True),
    StructField("sucesso", BooleanType(), True),
    StructField("registros_processados", LongType(), True),
    StructField("duracao_segundos", DoubleType(), True),
    StructField("mensagem_erro", StringType(), True),
    StructField("camada", StringType(), True),
    StructField("timestamp_execucao", StringType(), True)
])

if log_data:
    df_log = spark.createDataFrame(log_data, schema=schema)

    # Escrita Delta em modo Append para manter o hist√≥rico de execu√ß√µes
    write_data(df_log, "delta", "append", f"{schema_name}.{table_name}")
    
    print(f"‚úÖ {len(log_data)} logs persistidos com sucesso em {schema_name}.{table_name}")

    spark.sql("OPTIMIZE monitoramento.tb_monitoramento_log")
    spark.sql("VACUUM monitoramento.tb_monitoramento_log RETAIN 168 HOURS")
    
    print("‚úÖ OPTIMIZE e VACUUM executados com sucesso!!!")
    print("‚úÖ Monitoramento atualizado.")    

else:
    print("‚ö†Ô∏è Nenhum dado de log encontrado para processar.")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
